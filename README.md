# Langchain RAG with ollama Integration

## Overview

This project demonstrates how to integrate Ollama (LLM) with LangChain to build a simple retrieval-augmented generation (RAG) pipeline. It includes:

* A Python function to run the Ollama `llava` model via the Ollama CLI.
* LangChain-based code to:

  * Embed a set of text records using Hugging Face embeddings.
  * Store embeddings in a FAISS vector store.
  * Perform similarity search and invoke Ollama to generate answers based on retrieved documents.
* Examples of handling image-based questions using the `bakllava` model.

## Prerequisites

* **Operating System:** Linux, macOS, or Windows (with WSL2 for a Unix-like environment).
* **Python:** Version 3.8 or above.
* **Ollama CLI:** Installed and configured on your machine. See [Ollama Download](https://ollama.com/download) for details.
* **Ollama Models:** You should pull the necessary models (e.g., `llava`, `llama3`, `bakllava`) before running the code.

## Installation

### 1. Clone this repository


### 2. Install Ollama CLI

Visit [https://ollama.com/download](https://ollama.com/download) to find platform-specific installers. The quickest cross-platform method is using the official install script:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

This script will detect your operating system (Linux, macOS, or Windows/WSL2) and install the `ollama` binary accordingly. Follow any on-screen prompts.

Alternatively, download the installer directly:

* **macOS:** Requires macOS 11 Big Sur or later. Download the `.dmg` from the download page, open it, and drag `Ollama` to your `Applications` folder.
* **Windows (Preview):** Requires Windows 10 or later. Download the `.exe` from the download page and run the installer. Note: Ollama on Windows uses WSL2; the installer may prompt you to install or configure WSL2.
* **Linux:** You can use the install script above or download the `.tgz` package:

  ```bash
  curl -fsSL https://ollama.com/download/ollama-linux-amd64.tgz | sudo tar zx -C /usr/local/bin
  ```

After installation, verify with:

```bash
ollama --version
```


### 3. Install Python dependencies

You can install the required packages using `pip`. Below is a list of dependencies derived from the codebase.

```bash
pip install langchain          # Core LangChain library
pip install langchain-community  # For Ollama integration
pip install faiss-cpu          # FAISS vector store (or faiss-gpu if you have GPU support)
pip install sentence-transformers # Hugging Face embedding model
pip install transformers       # For Hugging Face models
pip install pillow             # For image handling (PIL)
```

## Dependencies

* **Python Standard Library:** `subprocess`, `os`, `base64`, `io`
* **LangChain:** `langchain`, `langchain-community` (for the `Ollama` LLM wrapper)
* **Vector Store:** `faiss-cpu` (or `faiss-gpu`)
* **Embeddings:** `sentence-transformers` (e.g., `all-MiniLM-L6-v2`)
* **Transformers:** `transformers` (used indirectly by `HuggingFaceEmbeddings`)
* **Pillow:** `Pillow` (for loading and converting images)

## Setting Up Data

1. **Patient Records File**

   * Create a plaintext file named `patient_records.txt` in the project root.
   * Format: Separate each patient record by a blank line. Example:

     ```text
     John Doe, 45, Chest Pain, Hospital A:555-1234
     Diagnosed with hypertension.

     Emily Brown, 30, Migraine, Hospital B:555-5678
     Prescribed pain medication.
     ```

2. **Images (Optional)**

   * To use the image-based LLM calls (with `bakllava`), place your JPEG/PNG images in the project directory. The code example uses paths like `./1576591134140.jpeg` and `high-low-temps_download5_2021.jpg`.


## Usage



### 1. LangChain + FAISS + Ollama Retrieval

* Ensure `patient_records.txt` is in the root directory.
* The script shows sample queries and prints responses generated by the `llava` model based on top-3 retrieved documents.
* Modify or add queries at the bottom of `langchain_retrieval.py` as needed.

### 2. Multi-Model Image Examples

* Place your test images (e.g., `1576591134140.jpeg`) in the project directory.
* The script converts images to base64 and invokes `bakllava`.
* Replace the hardcoded file paths with your own image paths.

## Customization

* **Change Prompt Templates:** Modify `PromptTemplate(template="{context}\n\nQuestion: {query}\nAnswer:", ...)` to adjust how retrieved context and the query are combined.
* **Adjust `k` (Number of Retrieved Documents):** In `retrieve_and_process`, change `k=3` to any number suitable for your use case.
* **Swap Embedding Models:** Edit `HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")` to another Hugging Face embedding model.
* **Use GPU-Accelerated FAISS:** If you have GPU support and installed `faiss-gpu`, replace `faiss-cpu` with `faiss-gpu` in your environment.

## Troubleshooting

* **Ollama Not Found:** If you get an error `Error running ollama: command not found`, ensure Ollama is installed and added to your system PATH. You can find installers at [https://ollama.com/download](https://ollama.com/download).
* **Model Not Pulled:** If Ollama cannot find the `llava` or `bakllava` models, run `ollama pull llava` or `ollama pull bakllava`.
* **Embedding Errors:** If Hugging Face embeddings fail, verify that `sentence-transformers` and `transformers` are installed and up to date.
* **FAISS Import Error:** If `import faiss` fails, install `faiss-cpu` or `faiss-gpu` with a compatible version.

## Tutorial

For a comprehensive guide on integrating Ollama as an LLM in LangChain, refer to the official LangChain documentation:

* [LangChain Ollama Integration](https://python.langchain.com/v0.2/docs/integrations/llms/ollama/)

This tutorial covers:

1. **Initialization:** How to configure the `Ollama` wrapper from `langchain_community.llms`.
2. **Model Invocation:** Examples of invoking prompts and handling responses.
3. **Advanced Usage:** Techniques for passing image contexts, setting custom parameters, and fine-tuning prompts.

### Quick Start Snippet

```python
from langchain_community.llms import Ollama

# Initialize Ollama with the desired model
llm = Ollama(model="llava")

# Simple text-based prompt
response = llm.invoke("Tell me a joke")
print(response)
```

### Image Context Example

```python
import base64
from io import BytesIO
from PIL import Image
from IPython.display import HTML, display
from langchain_community.llms import Ollama

# Load and convert image to base64
def convert_to_base64(pil_image):
    if pil_image.mode == 'RGBA':
        pil_image = pil_image.convert('RGB')
    buffered = BytesIO()
    pil_image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

file_path = "./image_example.jpg"
pil_image = Image.open(file_path)
image_b64 = convert_to_base64(pil_image)

# Bind image context to the model
llm_with_image = Ollama(model="bakllava").bind(images=[image_b64])

# Invoke with image context
print(llm_with_image.invoke("What is the object in this image?"))
```

For more details, parameter options, and best practices, consult the full tutorial:

[https://python.langchain.com/v0.2/docs/integrations/llms/ollama/](https://python.langchain.com/v0.2/docs/integrations/llms/ollama/)

## License

This project is released under the MIT License. See the [LICENSE](LICENSE) file for details.

---

*Happy coding!*
